{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce34dad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yesha\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\yesha\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"solving pendulum using actor crtic algorithm\"\"\"\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout,Input\n",
    "from keras.layers.merging import add,multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import random\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bf017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines how to assign values to each state, i.e, takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "class ActorCritic:\n",
    "    def __init__(self,env,sess):\n",
    "        self.env=env\n",
    "        self.sess=sess\n",
    "        \n",
    "        self.learning_rate=0.001\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_decay=0.995\n",
    "        self.gamma=0.95\n",
    "        self.tau=0.125\n",
    "        \n",
    "        #======================================================================#\n",
    "        #                         Actor Model                                  #\n",
    "        # chain rule: find the gradient of changing the actor network params in#\n",
    "        # getting closest to the final value network predictions, i.e. de/dA   #\n",
    "        # Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act#\n",
    "        #======================================================================#\n",
    "        \n",
    "        self.memory= deque(maxlen=2000)\n",
    "        self.actor_state_input,self.actor_model = self.create_actor_model()\n",
    "        _, self.target_actor_model = self.create_actor_model()\n",
    "        \n",
    "        self.actor_critic_grad=tf.placeholder(tf.float32,[None, self.env.action_space.shape[0]]) \n",
    "        # where will we feed de/dC (from critic)\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output,actor_model_weights, -self.actor_critic_grad)\n",
    "        # dC/dA from actor\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "        \n",
    "        #======================================================================#\n",
    "        #                              critic model                            #\n",
    "        #======================================================================#\n",
    "        \n",
    "        self.critic_state_input, self.critic_action_input, \\\n",
    "                self.critic_model=self.create_critic_model()\n",
    "        _, _, self.target_critic_model=self.create_critic_model()\n",
    "        \n",
    "        self.critic_grads = tf.gradients(self.critic_model.output,self.critic_action_input)\n",
    "        # where we calcaulte de/dC for feeding above\n",
    "        \n",
    "        # Initialize for later gradient calculations\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "    \n",
    "    # ========================================================================= #\n",
    "    #                              Model definations                            #\n",
    "    # ========================================================================= #\n",
    "    \n",
    "    def create_actor_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation='relu')(state_input)\n",
    "        h2 = Dense(48, activation='relu')(h1)\n",
    "        h3 = Dense(24, activation='relu')(h2)\n",
    "        output = Dense(self.env.action_space.shape[0], activation='relu')(h3)\n",
    "        \n",
    "        model = Model(state_input, output)\n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, model\n",
    "    \n",
    "    def create_critic_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        state_h1 = Dense(24, activation='relu')(state_input)\n",
    "        state_h2 = Dense(48)(state_h1)\n",
    "        \n",
    "        action_input = Input(shape=self.env.action_space.shape)\n",
    "        action_h1 = Dense(48)(action_input)\n",
    "        \n",
    "        merged    = add([state_h2, action_h1])\n",
    "        merged_h1 = Dense(24, activation='relu')(merged)\n",
    "        output = Dense(1, activation='relu')(merged_h1)\n",
    "        model  = Model([state_input,action_input],output)\n",
    "        \n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, action_input, model\n",
    "        \n",
    "        \n",
    "    # ========================================================================= #\n",
    "    #                               Model Training                              #\n",
    "    # ========================================================================= #\n",
    "    \n",
    "    \n",
    "    def remember(self, cur_state, action, reward, new_state, done):\n",
    "        self.memory.append([cur_state, action, reward, new_state, done])\n",
    "        \n",
    "    def _train_actor(self,samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "            predicted_action = self.actor_model.predict(cur_state)\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict={self.critic_state_input:  cur_state,self.critic_action_input: predicted_action})[0]\n",
    "            self.sess.run(self.optimize, feed_dict={self.actor_state_input: cur_state,self.actor_critic_grad: grads})\n",
    "            \n",
    "    def _train_critic(self,samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(new_state)\n",
    "                future_reward = self.target_critic_model.predict([new_state, target_action])[0][0]\n",
    "                reward += self.gamma * future_reward\n",
    "            self.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "            \n",
    "    def train(self):\n",
    "        batch_size=32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        rewards=[]\n",
    "        samples=random.sample(self.memory,batch_size)\n",
    "        self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "        \n",
    "    # ========================================================================= #\n",
    "    #                          target model updating                            #\n",
    "    # ========================================================================= #\n",
    "    \n",
    "    \n",
    "    def _update_actor_target(self):\n",
    "        actor_model_weights  = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_critic_model.get_weights()\n",
    "        \n",
    "        for i in range(len(actor_target_weights)):\n",
    "            actor_target_weights[i] = actor_model_weights[i]\n",
    "        self.target_critic_model.set_weights(actor_target_weights)\n",
    "        \n",
    "    def _update_critic_target(self):\n",
    "        critic_model_weights  = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.critic_target_model.get_weights()\n",
    "        \n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = critic_model_weights[i]   \n",
    "        critic_target_weights[i] = critic_model_weights[i]\n",
    "        \n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "        \n",
    "    \n",
    "    # ========================================================================= #  \n",
    "    #                             Model Predictions                             #\n",
    "    # ========================================================================= #\n",
    "    \n",
    "    \n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.actor_model.predict(cur_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ad18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    env= gym.make(\"Pendulum-v1\")\n",
    "    actor_critic = ActorCritic(env, sess)\n",
    "    \n",
    "    num_trials = 10000\n",
    "    trial_len  = 500\n",
    "    \n",
    "    cur_state = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    while(True):\n",
    "        env.render()\n",
    "        cur_state=cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "        action = actor_critic.act(cur_state)\n",
    "        action = action.reshape((1, env.action_space.shape[0]))\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "        \n",
    "        actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "        actor_critic.train()\n",
    "        \n",
    "        cur_state=new_state\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "        main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe456e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
